{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information from the Web Snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web snapshots of Google Scholar searches were saved to a Zotero Group Library. Each snapshot is a single page of the search results.\n",
    "\n",
    "From each of the saved web snapshots extract the following:\n",
    "\n",
    "- Title\n",
    "- First author\n",
    "- Number of citations\n",
    "- Zotero key\n",
    "- Publication year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, basename, splitext\n",
    "import re\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pyzotero import zotero\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from lib.secrets import WEB_SNAPSHOTS, USER_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = join('data', 'attachments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Zotero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect to this shared Zotero library of `Web Snapshots` of Google Scholar searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zot = zotero.Zotero(WEB_SNAPSHOTS, 'group', USER_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through Zotero Library and Save the Web Pages to a Work Directory (Zip File)\n",
    "\n",
    "Given a collection of Google Scholar searches in a Zotero collection library we want to get the saved Google Scholar web page from that search. In this case the Google Scholar searches were saved in a group library called `Web Snapshots`. Within that library there are several collections. Each collection represents the Google Scholar web pages returned by a particular search term like \"biodiversity database\". Each search term may return multiple pages. There will be one attachment in the collection for each web page. So there will be one attachment in the collection for the first page returned from the\" biodiversity database\" search (results 1-10) and another attachment for the the second page (11-20), and so on.\n",
    "\n",
    "Here we:\n",
    "\n",
    "1) Loop through every search term (or collection).\n",
    "\n",
    "2) For every search term we loop through every page (or attachment) returned by that search term.\n",
    "\n",
    "3) We download the attachment which be a zip file that will contain data than we need for this notebook. So, in the next section we will extract what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attachments = []\n",
    "\n",
    "for collection in zot.collections():\n",
    "    attachments = [a for a\n",
    "                   in zot.everything(zot.collection_items(collection['key']))\n",
    "                   if a['data']['itemType'] == 'attachment']\n",
    "    for attachment in tqdm(attachments, desc='attachments'):\n",
    "        all_attachments.append(attachment)\n",
    "        zot.dump(attachment['key'], '{}'.format(attachment['key']), output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the HTML Web Pages from the Zip Files\n",
    "\n",
    "The zip files from the previous step contain more data than we need. We want to extract just the web page from the search. Here we loop through all of the zip files and extract the search web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "zip files: 100%|██████████| 183/183 [00:00<00:00, 867.39it/s]\n"
     ]
    }
   ],
   "source": [
    "pattern = join(output_dir, '*.zip')\n",
    "target = 'scholar.html'\n",
    "src = join(output_dir, target)\n",
    "\n",
    "zip_files = glob(pattern)\n",
    "for zip_file in tqdm(zip_files, desc='zip files'):\n",
    "\n",
    "    with ZipFile(zip_file) as zippy:\n",
    "        name_list = zippy.namelist()\n",
    "\n",
    "        if target not in name_list:\n",
    "            continue\n",
    "\n",
    "        zippy.extract(target, output_dir)\n",
    "\n",
    "        base_name = basename(zip_file)\n",
    "        file_name = splitext(base_name)[0]\n",
    "        dst = join(output_dir, '{}.html'.format(file_name))\n",
    "\n",
    "        os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data from HTML Pages\n",
    "\n",
    "Now we can extract the data from the saved web pages. We're using a Python library called BeautifulSoup4 for this.\n",
    "\n",
    "Fields extracted from each search result:\n",
    "* Title\n",
    "* First author\n",
    "* Publication year\n",
    "* Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "html files: 100%|██████████| 183/183 [00:18<00:00,  9.30it/s]\n"
     ]
    }
   ],
   "source": [
    "pattern = join(output_dir, '*.html')\n",
    "html_files = glob(pattern)\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for html_file in tqdm(html_files, desc='html files'):\n",
    "\n",
    "    with open(html_file) as in_file:\n",
    "        page = in_file.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    base_name = basename(html_file)\n",
    "    key = splitext(base_name)[0]\n",
    "\n",
    "    for result in soup.select('div.gs_r'):\n",
    "\n",
    "        # Title\n",
    "        title_obj = result.select_one('.gs_rt a')\n",
    "        if not title_obj:\n",
    "            continue\n",
    "\n",
    "        title = title_obj.get_text()\n",
    "\n",
    "        # This contains several fields which are extracted later\n",
    "        author_string = result.select_one('.gs_a').get_text()\n",
    "\n",
    "        # First author\n",
    "        authors = author_string.split('-')[0]\n",
    "        first_author = authors.split(',')[0].strip()\n",
    "\n",
    "        # Publication year\n",
    "        match = re.search('\\d{4}', author_string)\n",
    "        publication_year = match.group(0) if match else ''\n",
    "\n",
    "        # Citations\n",
    "        citation_string = result.find(text=re.compile(r'Cited by \\d+'))\n",
    "        citations = 0\n",
    "        if citation_string:\n",
    "            match = re.search('\\d+', citation_string)\n",
    "            citations = int(match.group(0)) if match else '0'\n",
    "\n",
    "        all_docs.append({\n",
    "            'key': key,\n",
    "            'title': title,\n",
    "            'first_author': first_author,\n",
    "            'publication_year': publication_year,\n",
    "            'citations': citations})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = join(output_dir, 'citations_v3.csv')\n",
    "\n",
    "df = pd.DataFrame(all_docs)\n",
    "df.shape\n",
    "df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
